{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Before we can attempt to implement a model, we must perform exploratory data analysis. This will allow us to better understand the data that was given to us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "raw_data = pd.read_csv(\"../data/ucsbdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial observations \n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Observations and missing values\n",
    "print(\"Number of observations:\", len(raw_data.Index))\n",
    "print(\"Number of variables:\", len(raw_data.columns))\n",
    "print(\"Number of missing values:\", np.count_nonzero(raw_data.isnull()))\n",
    "print(\"Number of observations with missing values:\", raw_data.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return df will all the missing values\n",
    "raw_data[raw_data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a significant amount of missing values in the dataset. The first year with complete observations is 2008. This would allow us to create a model after the crash of the 2008 recession. However, if we wanted to work with years prior to 2008, we need to further analyze the missing values in the dataset. \n",
    "\n",
    "## Post Market Crash\n",
    "\n",
    "The most recent US recession occerd in 2008. September 29, 2008 was the day the stock market crashed. Since this infamous day, there have been no such significant drop or gains with the stocks. We will assume the stock will not crash during this time of the contest. Our first model will consist of data from September 29, 2008 to the end of 2018. This will roughly give us 10 years worth of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset after stock market crash\n",
    "initial_start = '2008-08-30'\n",
    "stock_data1 = raw_data.loc[raw_data.Index > initial_start]\n",
    "stock_data1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Our data is considered high dimension since it consists of 67 varaibles plus our target variable of prediction. In order for our model to run effectivly, we need to only select the varaibles that have the most influence in predicitng our desired variable. We will attempt various feature selection models to select the variable for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Data Dates and store dates\n",
    "data_dates = stock_data1['Index']\n",
    "data_dates.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data dates from our current complete dataset\n",
    "no_dates = stock_data1.drop(columns=['Index'])\n",
    "no_dates.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable that we are interested in predicting\n",
    "actual_returns = stock_data1['R']\n",
    "actual_returns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature variables\n",
    "feature_variables = stock_data1.drop(columns = ['Index', 'R'])\n",
    "feature_variables.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearsons Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of varialbes that we want \n",
    "number_features = 8\n",
    "fr = SelectKBest(score_func = f_regression, k = number_features)\n",
    "pearsons_features = fr.fit_transform(feature_variables, actual_returns)\n",
    "np.savetxt('../data/pearsons_features.txt', pearsons_features, fmt = '%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Instead of finding specific individual features to use, we will use components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training set and test set for PCA\n",
    "train_data, test_data, train_lbl, test_lbl = train_test_split(feature_variables, \n",
    "                                                             actual_returns.values,\n",
    "                                                             test_size = 0.2, \n",
    "                                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Class to apply standard transformation on data\n",
    "scaler = StandardScaler()\n",
    "# Fit ; Compute the mean and std to be used for later scaling\n",
    "scaler.fit(train_data)\n",
    "# Apply standard transformaiton to test and training data\n",
    "transformed_train_data = scaler.transform(train_data)\n",
    "transformed_test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA class \n",
    "pca = PCA(svd_solver='full')\n",
    "# Fit PCA to trainig set\n",
    "pca.fit(transformed_train_data)\n",
    "# Apply PCA to both training and test data\n",
    "pca_train_data = pca.transform(transformed_train_data)\n",
    "pca_test_data = pca.transform(transformed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skree plot will help us the number of components to use in our models. We will looking for the last major drop in explained variance. We want to obtain the number of personal components before the skree plot levels out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skree plot for PCA\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x = range(66) , y = pca.explained_variance_ratio_,\n",
    "            color = \"purple\")\n",
    "plt.title(\"Skree Plot for PCA Components\", size = 20)\n",
    "plt.ylabel(\"Explained Variance\", size = 15)\n",
    "plt.xlabel(\"Number of components\", size = 15)\n",
    "plt.xticks(np.arange(0, 66, 2));\n",
    "plt.vlines(x = 8, ymin = 0, ymax = 0.35, colors = 'red', linestyles='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components\n",
    "target_component = 8\n",
    "# Select only first 8 component from data\n",
    "pca_train_data = pca_train_data[:,0: target_component]\n",
    "pca_test_data = pca_test_data[:, 0: target_component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA results\n",
    "np.savetxt('../data/pca_train_data.txt', pca_train_data, fmt = '%f')\n",
    "np.savetxt('../data/pca_test_dat.txt', pca_test_data, fmt = '%f')\n",
    "# Save labels\n",
    "np.savetxt('../data/train_labels.txt', train_lbl, fmt = '%f')\n",
    "np.savetxt('../data/test_labels.txt', test_lbl, fmt = '%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
